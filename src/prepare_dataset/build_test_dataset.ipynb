{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-25T12:49:18.793776Z",
     "start_time": "2024-07-25T12:49:18.784672Z"
    }
   },
   "source": [
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast"
   ],
   "execution_count": 59,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T12:49:18.808692Z",
     "start_time": "2024-07-25T12:49:18.804257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "files_path = r'../../temp/test_dataset.xlsx'\n",
    "# df = pd.read_excel('../../temp/test_dataset.xlsx')"
   ],
   "id": "696c8658aa5ec48c",
   "execution_count": 60,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T12:49:18.855854Z",
     "start_time": "2024-07-25T12:49:18.849151Z"
    }
   },
   "cell_type": "code",
   "source": "# df['content'] = df['content'].apply(ast.literal_eval)",
   "id": "e9e96dd6cbdc3713",
   "execution_count": 61,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T12:49:18.871810Z",
     "start_time": "2024-07-25T12:49:18.856808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# df['content'] = df['content'].apply(lambda x: ''.join(x))\n",
    "# df['content']"
   ],
   "id": "bac3237d4b7a0faf",
   "execution_count": 62,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T12:49:18.886529Z",
     "start_time": "2024-07-25T12:49:18.874228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# df = df[['content']]\n",
    "# df.to_excel('../../temp/pre_test_dataset.xlsx', index=False)"
   ],
   "id": "930a42d879f11b71",
   "execution_count": 63,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T12:49:18.917222Z",
     "start_time": "2024-07-25T12:49:18.895650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NewsAnalysis:\n",
    "    def __init__(self, files_path):\n",
    "        self.files_path = files_path\n",
    "        self.data = None\n",
    "        \n",
    "        self.max_relations_per_row  = 10\n",
    "\n",
    "    def read_data(self):\n",
    "        # 读取Excel文件\n",
    "        self.original_data = pd.read_excel(self.files_path, sheet_name='Sheet2')\n",
    "        self.data = self.original_data.copy()\n",
    "\n",
    "    def transform_content(self):\n",
    "        # 转换content列中的字符串表示的list为实际的list，然后合并为字符串\n",
    "        self.data['content'] = self.data['content'].apply(ast.literal_eval)\n",
    "        self.data['content'] = self.data['content'].apply(lambda x: ','.join(x))\n",
    "        \n",
    "    def find_closest_index(self, content, position_names, target_name):\n",
    "        # 找到最接近的职位索引\n",
    "        min_distance = float('inf')\n",
    "        best_match = (-1, -1)  # 存储最佳匹配的起始和结束索引\n",
    "        for position in position_names:\n",
    "            if position in content:\n",
    "                pos_index = content.index(position)\n",
    "                name_index = content.index(target_name)\n",
    "                distance = abs(pos_index - name_index)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    start_idx = pos_index\n",
    "                    end_idx = pos_index + len(position) - 1\n",
    "                    best_match = (start_idx, end_idx)\n",
    "        if best_match == (-1, -1):\n",
    "            print(f\"未找到匹配的职位'{position_names}'接近'{target_name}'\")\n",
    "        \n",
    "        return best_match\n",
    "\n",
    "    def analyze_positions(self):\n",
    "        # 初始化存储分析结果的列表\n",
    "        max_relations_per_row = self.max_relations_per_row\n",
    "        for i in range(1, max_relations_per_row + 1):\n",
    "            self.data[f'实体关系{i}'] = None  # 添加足够的列来存储关系\n",
    "        \n",
    "        # 初始化存储分析结果的列表\n",
    "        results = []\n",
    "        previous_content = None  # 用于存储上一行的内容\n",
    "        \n",
    "        # 遍历每一行\n",
    "        for index, row in self.data.iterrows():\n",
    "            content = row['content']\n",
    "            if pd.isna(content) and previous_content is not None:\n",
    "                # 如果当前行的content为空，则使用上一行的content\n",
    "                content = previous_content\n",
    "                print(f\"Warning: Row {index} has empty content, use previous content\")\n",
    "            elif pd.isna(content):\n",
    "                # 如果连续行的content为空，则跳过\n",
    "                continue\n",
    "                \n",
    "            entities = [\n",
    "                ('来访城市', row.get('来访城市'), '来访人姓名', row.get('来访人姓名'), '来自'),\n",
    "                ('来访人职位', row.get('来访人职位'), '来访人姓名', row.get('来访人姓名'), '担任'),\n",
    "                ('受访城市', row.get('受访城市'), '接访人姓名', row.get('接访人姓名'), '来自'),\n",
    "                ('接访人职位', row.get('接访人职位'), '接访人姓名', row.get('接访人姓名'), '担任'),\n",
    "                ('来访人姓名', row.get('来访人姓名'), '接访人姓名', row.get('接访人姓名'), '接待')\n",
    "            ]\n",
    "\n",
    "            relation_set = set()\n",
    "            # 遍历实体对\n",
    "            for entity1_type, entity1_value, entity2_type, entity2_value, relation in entities:\n",
    "                if pd.notna(entity1_value) and pd.notna(entity2_value) and entity1_value in content and entity2_value in content:\n",
    "                    start_idx1 = content.index(entity1_value)\n",
    "                    end_idx1 = start_idx1 + len(entity1_value) - 1  \n",
    "                    start_idx2 = content.index(entity2_value) - 1\n",
    "                    end_idx2 = start_idx2 + len(entity2_value)\n",
    "                    results.append(f\"{{[{start_idx1},{end_idx1}],{entity1_value}}}, {{[{start_idx2},{end_idx2}],{entity2_value}}},{relation}\")\n",
    "\n",
    "                    # 特殊处理职位，去除城市名称后重新寻找\n",
    "                    if '职位' in entity1_type:\n",
    "                        possible_positions = [entity1_value, entity1_value.replace(row.get('受访城市', '') + '市', '').strip()]\n",
    "                        start_idx1, end_idx1 = self.find_closest_index(content, possible_positions, entity2_value)\n",
    "\n",
    "                    relation_desc = f\"{{[{start_idx1},{end_idx1}], {entity1_value}}}, {{[{start_idx2},{end_idx2}], {entity2_value}}}, {relation}\"\n",
    "                    if relation_desc not in relation_set:\n",
    "                        relation_set.add(relation_desc)\n",
    "                        \n",
    "                else:\n",
    "                    print(f\"在content中未找到 '{entity1_value}' 或 '{entity2_value}'。\")\n",
    "            \n",
    "            # 存储关系到列\n",
    "            for i, relation in enumerate(relation_set):\n",
    "                if i < max_relations_per_row:\n",
    "                    self.data.at[index, f'实体关系{i+1}'] = relation\n",
    "                    \n",
    "            previous_content = content  # 更新上一行的内容为当前行\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def save_results(self, save_path=r'../../temp'):\n",
    "        self.data = self.data[['content'] + [f'实体关系{i+1}' for i in range(self.max_relations_per_row)]]\n",
    "        self.data = self.data.rename(columns={'content': '文本内容'})\n",
    "        \n",
    "        # 保存分析结果\n",
    "        self.data.to_excel(os.path.join(save_path, 'final_test_dataset.xlsx'), index=False)\n",
    "        \n",
    "        print(f\"Analysis results saved to {os.path.join(save_path, 'final_test_dataset.xlsx')}\")\n",
    "        \n"
   ],
   "id": "9cf171770f3bb7db",
   "execution_count": 64,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T12:49:19.458943Z",
     "start_time": "2024-07-25T12:49:18.932682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 示例用法\n",
    "# files_path = '/path/to/your/excel.xlsx'\n",
    "news_analysis = NewsAnalysis(files_path)\n",
    "news_analysis.read_data()\n",
    "news_analysis.transform_content()\n",
    "results = news_analysis.analyze_positions()\n",
    "for result in results:\n",
    "    print(result)\n",
    "news_analysis.save_results()"
   ],
   "id": "4efe98c7baf2979d",
   "execution_count": 65,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T11:17:24.542076Z",
     "start_time": "2024-07-28T11:17:24.526026Z"
    }
   },
   "cell_type": "code",
   "source": "import paddle",
   "id": "f3314d83ed60a7ed",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T11:17:26.199044Z",
     "start_time": "2024-07-28T11:17:26.087905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "paddle.utils.run_check()\n",
    "paddle.set_device('gpu')"
   ],
   "id": "a4fe2faa4c40ba28",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "96e69fc43669252a",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
